# Data Engineering Portfolio — 4 Job‑Ready Projects

This package contains **4 complete, runnable data engineering projects** designed to demonstrate end‑to‑end skills:
- ingestion (batch + streaming), transformation (SQL/dbt), orchestration (Airflow), lakehouse patterns (Delta/Iceberg style), data quality (Great Expectations), CI, and reproducible environments (Docker).

> Tip: Each project runs locally with Docker. Follow each project's README for exact commands.

## Projects
1. **01_batch_etl_dbt_airflow** — Raw → Staging → Mart using Postgres + dbt + Airflow (with tests + documentation).
2. **02_streaming_kafka_spark_delta** — Real‑time clickstream aggregation using Kafka + Spark Structured Streaming + Delta Lake (local).
3. **03_data_quality_observability** — Great Expectations suite + automated checks + quality reporting.
4. **04_lakehouse_iac_aws_style_local** — Lakehouse layout + IaC (Terraform) + local execution patterns (duckdb + parquet) mirroring AWS Glue/Athena approach.

## What to showcase on LinkedIn/GitHub
- Screenshots: Airflow DAG success, dbt docs site, Spark streaming console output + Delta tables, GE validation report.
- Evidence: CI passing, unit tests, data lineage diagrams (Mermaid).

## Quick start
Install Docker Desktop, then pick a project folder and follow its README.

## License
MIT — use freely in your portfolio.
